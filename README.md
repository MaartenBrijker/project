# DEVORM RECORDS presents an user-interactive music release

For the record label DEVORM, I will build an app that generates music on the basis of user made choices. 
Hereby DEVORM makes it possible for fans and consumers to not only purchase/listen to the music we put out, but now also to be part of the label. 
The music that will be generated by the user will be unique, but still be constrained by the tools DEVORM provides and the aesthetics build into the app.
To state this more explicetly, this app will not be a generetic tool to make music, but will have a skin of DEVORM layered on top of this and released as an art project by DEVORM.

# How does it work

The app will exist of three parts:
 - input
 - audio processing
 - output

Most of the focus in these four weeks will be on the audio processing part. The input and output parts will have some amount of implementation levels that could possibly be skipped during these four weeks and left for further work.

I will now explain each part of the app more detailed.

# input

The app is able to take input from several sources:
 - web (download music from youtube or soundcloud for example)
 - database (a set of sounds provided by me, stored in the app)
 - record audio (recording audio with the device's mic)

I will start building the app with only the database source of audio. I have done this before (the API assignment last block), and that will be an easy start for focussing on the most important part of the app, namely the audio processing

# audio processing

For this part of the app I will use the Audio Unit API. This API makes use of the Core Audio build in to swift, but is more accessable and easier to work with. Audio Unit provides several samplers, effects and sequencers to manipulate audio. Also it has some graphical tools I am planning on using. 

# output

In the most narrowed down version of my app, sound will only be 'outputted' in terms of playing the audio real time. So when the user kills the app, the audio will be gone. However, in a more extended version of this app I would like the audio to be exported as a single audio file, so the music can be stored and shared by the user. The AVAssetExportSession function of iOS would be able to do this. Even more fancy would be if the user could have the audio send to him via mail (with a cc to me, so DEVORM could display all the generated audio in some online environment) or streamed on Soundcloud. 
However I don't expect to be able to get to this part in this four weeks course. So all of this is optional and I will work foremostly with just playing back the audio real time.

# multiple screens

I will make use of a splitview. Were the main view is a table displaying all the audio sources, and the detailview is filled with several audio processors (sliders, buttons), a visual representation of the audio and a function to select playbacking modes (single sound vs overall sound)

![alt-tag](https://github.com/MaartenBrijker/project/blob/master/doc/sketch.png)

Ideally there will also be another screen where users could control a mixer to control overall volumes and panning of each track.

# sound reference

Ideally the sonic aesthetics users are able to generate with this app would be in line of this song: https://www.youtube.com/watch?v=ZMTT1Pnfuzw

# MVP (minimal viable product)

As already implicitly stated, my MVP will consist of an audioprocessing app using a offline sound database (provided by me) and only outputting audio in real time playback (not exporting any audio to files).

# technical limitations

I mostly foresee technical limitations in the audio input and output parts. However that's why I have chosen to limit my MVP and focus foremostly on the audio processing part. Technical limitations in those parts include obtaining audio from an online source and writing audio to a file and storing that online. 
With respect to the time and my personal skill level I don't foresee myself hardcoding processing functions. So in the audio processing part of the app technical limitations would be based on the audio processing functions provided by the Audio Unit API. I will have to be creative to use these processing tools in a way that approaches the functions and aesthetics I have in mind and that will be satisfactionary to the user of the app.
Especially the part where I would provide users the possibility to for example place samples in a time based grid or automate effects over time will be hard to implement. Actually, everything that is time-based will be harder to implement, like beat-matching different tracks etc. I will first try to use only "beatless" tracks in the process.

# what does this app add to the world of apps?

There are several music generating and processing applications around (e.g. Multitrack, Garageband, Figure to name a few). However non of them is aesthetically limited by a record label and does provide the direct possibility of being part of a collective group of artists without having any necessary musical skills.

This app will enable music fans that are not capable of composing music from scratch, to still have a music based output. A creative outlet that even transcends the fan to the level of being part of a music label, to enter a creative dialogue, and to build a sense of sonic community (if I would implement audio exporting features to a shared soundcloud for example). 